{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# the main focus of this notebook is on dimesionality reduction. It makes the code faster, more physically understandable, and more predictive.\n# this topic is a book by itself, but simple tools can be used to get rid of features that only make the model more complex but not more predictive.\n# here, I mention a few simple one:\n# if more than 25% of each feature data is missing, we can drop it\n# if std of each feature is very low, here < 0.005, we may drop it\n# if the pairwise correlation among features is more than 75%, we may drop one of them\n# if VIF among features is more than 5, we may drop one feature\n# Decision Tree algorithm can find the importance of each feature for the target, we can set a threshold and ingnore less important ones\n\n# not to mention, before this section of the code, we already drop variables that:\n# have very low correlation with the target\n# have more than 0.05 p_value\n# being a categorical feature, have high ratio of value_counts to the length of dataset","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport sklearn as sk\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cufflinks as cf\nfrom scipy import stats\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import f_regression\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.datasets import make_classification\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import learning_curve, GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.feature_selection import RFE\nfrom sklearn.pipeline import Pipeline\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential, load_model\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping\n\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\npd.set_option('display.width', None)\npd.set_option('display.max_colwidth', None)\n\ncf.go_offline()\nsns.set()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/loan-predication/train_u6lujuX_CVtuZ9i (1).csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1) Defining the target","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Loan_Status'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# first make sure target or label is categorical: our target is 1 if paid, and 0 if not\ndf['target'] = df['Loan_Status'].map({'Y' : 1, 'N' : 0})\ndf.drop('Loan_Status', axis = 1, inplace = True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop(['Loan_ID'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# percentage of missing values for each column:\n(100 * (df.isnull().sum()) / len(df)).sort_values(ascending = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2) droping clearly useless features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# this finds number of unique values for each categorical feature, if high, one can drop that specific feature\ncol_list = []\nfactor = []\nnu = []\n\nfor col in df.select_dtypes(['object']):\n    col_list.append(col)\n    factor.append(100 * df[col].nunique() / len(df))\n    nu.append(df[col].nunique())\n\ncol_list = np.array(col_list).T\nfactor = np.array(factor).T\nnu = np.array(nu).T\n\nfactor_df = pd.DataFrame(data = col_list, columns = ['Column'])\nfactor_df['Factor'] = factor\nfactor_df['nu'] = nu\nfactor_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# above numbers look to be pretty small, so we keep them all","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3) what features are not important for this target","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"np.abs(df.corr()['target']).sort_values(ascending = True)[:-1].plot.bar(figsize = (16,8))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.abs(df.corr()['target']).sort_values(ascending = True)[:6].index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's drop the 'ApplicantIncome'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we can also look into the p_values\n\ncol_list = []\np_list = []\nfor col in df.select_dtypes(['number']):\n    slope, intercept, r_value, p_value, std_err = stats.linregress(df[col], df['target'])\n    col_list.append(col)\n    p_list.append(p_value)\n    #print(f'{col} is associated with the target wtih p_value of:    {p_value}')\n\npval_table = pd.DataFrame(data = col_list, columns = ['col'])\npval_table['p_values'] = p_list\npval_table.sort_values(by = 'p_values', ascending = False)\n\n# based on here and p_value limit of 0.05, we can ignore first six ones, here we keep all","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# above, again suggest we can drop 'ApplicantIncome'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop('ApplicantIncome', axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now lets look into target dependance on categorical features\nfor col in df.select_dtypes(['object']):\n    plt.figure(figsize = (16,6))\n    sns.countplot(df[col], hue = df['target'])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# looks they all are important in the target","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4) look into each categorical column","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# firsr lets look into each object column\nfor col in df.select_dtypes(['object']):\n    print()\n    print('for the feature:     ', col)\n    print(df[col].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5) Dealing with missing values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# total percentage of missing data\n100 * (df.isnull().sum().sum()) / len(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets delete them\ndf = df.dropna()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6) is target balanced?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# is the label balanced?\nsns.countplot(x = 'target', data = df)\n\n# it is unbalanced but not too bad, as we dont have that many data, lets keep all","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# balance the input for target, if only there are to values 0 and 1 as in val1 and val2\n\n# first shuffle indices:\ndf = df.sample(frac=1).reset_index(drop=True)\n\n# then set val1 and val2 below:\nval1 = 0\nval2 = 1\n\nval_1_bigger = 0\nval_2_bigger = 0\n\n# find the number of each label\nnum_val1 = df[df['target'] == val1].shape[0]\nnum_val2 = df[df['target'] == val2].shape[0]\n\nif num_val1 > num_val2:\n    val_1_bigger = 1\nelse:\n    val_2_bigger = 1\n\ni_1_count = 0\ni_2_count = 0\nindices_to_remove = []\n\nfor i in np.arange(len(df)):\n    \n    if df['target'][i] == 0:\n        i_1_count = i_1_count + 1\n        if (val_1_bigger == 1) and (i_1_count > num_val2):\n            indices_to_remove.append(i)\n    if df['target'][i] == 1:\n        i_2_count = i_2_count + 1\n        if (val_2_bigger == 1) and (i_2_count > num_val1):\n            indices_to_remove.append(i)\n\n            \ndf = df.drop(index=indices_to_remove, axis = 0)\ndf = df.reset_index(drop = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# is the label balanced, now?\nsns.countplot(x = 'target', data = df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in df.select_dtypes(['object']):\n    dummies = pd.get_dummies(df[col], drop_first = True, prefix = col)\n    df = pd.concat([df, dummies], axis = 1)\n    df = df.drop(col, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# dimensionality reduction","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dfcopy = df.drop('target', axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# a) based on percent missing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ther = 25","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# here we omit if more than 25% of a column is missing, the \"cols_to_be_deleted\" collects name of columns to be deleted\ncols_to_be_deleted = []\nfor col in dfcopy.columns:\n    per = 100 * (dfcopy[col].isnull().sum()) / len(dfcopy)\n    if per > ther and col not in cols_to_be_deleted:\n        print(col)\n        cols_to_be_deleted.append(col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_to_be_deleted","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# b) based on low variation in features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ther = 0.05","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# here we will drop columns that have std of below 0.005\nstd_cols = []\nfor col in dfcopy.columns:\n    std_col = df[col].std()\n    std_cols.append(std_col)\n    if std_col < ther and col not in cols_to_be_deleted:\n        cols_to_be_deleted.append(col)\n    \n\ndf_std = pd.DataFrame(data = std_cols, index = dfcopy.columns, columns = ['var'])\n#df_std.sort_values(by = 'var')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_to_be_deleted","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# c) based on pairwsie correlation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ther = 0.50","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfcorr = dfcopy.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# here if there is pairwise correlation above 0.75, we omit one of the columns to be added to the list\nfor i,col1 in enumerate(dfcorr.columns):\n    for j, col2 in enumerate(dfcorr.columns):\n        if j > i:\n            corr_cell = dfcorr.loc[col1, col2]\n            if corr_cell > ther and col2 not in cols_to_be_deleted:\n                cols_to_be_deleted.append(col2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_to_be_deleted","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# d) based on multi-co-lineatiry","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ther = 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# vif of more than 5 means multi-co-linearity\nfor i,col1 in enumerate(dfcorr.columns):\n    for j, col2 in enumerate(dfcorr.columns):\n        if j > i:\n            slope, intercept, r_value, p_value, std_err = stats.linregress(df[col1], df[col2])\n            vif = 1 / (1 - r_value * r_value)\n            if vif > ther and col2 not in cols_to_be_deleted:\n                cols_to_be_deleted.append(col2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_to_be_deleted","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# e) based on decision tree feature importance","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ther = 0.05","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = dfcopy\ny = df['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create decision tree classifer object\nclf = RandomForestClassifier()\n\n# Train model\nmodel = clf.fit(X, y)\n\n# Calculate feature importances\nimportances = model.feature_importances_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_importance = pd.DataFrame(data = importances, index = dfcopy.columns, columns = ['Importance'])\ndf_importance.sort_values(by = 'Importance').plot.barh(figsize = (12,24))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for ind in df_importance.index:\n    imp = df_importance.loc[ind]['Importance']\n    if imp < ther and col2 not in cols_to_be_deleted:\n        cols_to_be_deleted.append(ind)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_to_be_deleted","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop(cols_to_be_deleted, axis = 1)\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# setting up x and y, the .values make it a numpy array to put into tf\n\nx = df.drop('target', axis = 1)\ny = df['target']\n\n\n# split, first into train and test\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.1, random_state = 101)\n\n## scaling must happen after test_train split to avoid data leakage\n\nscalar = StandardScaler()\n\nx_train = scalar.fit_transform(x_train)\n\nx_test = scalar.transform(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# modeling and priting off coefs:\n\nlm = LogisticRegression()\nlm.fit(x_train, y_train.ravel())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predictions\n\npredictions = pd.DataFrame(lm.predict(x_test), columns = ['Predicted Values'])\npredictions ['Real Values'] = y_test.reset_index(drop = True)\npredictions.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# making a data frame for real vs predicted vs residuals\npredictions ['Residuals'] = predictions ['Real Values'] - predictions['Predicted Values']\nsns.countplot(predictions['Residuals'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, predictions['Predicted Values']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_cm = confusion_matrix(y_test, predictions['Predicted Values'])\nconf_temp = {'Predicted NO': [my_cm[0][0], my_cm[1][0]], 'Predicted YES': [my_cm[0][1], my_cm[1][1]] }\nmy_cmdf = pd.DataFrame(conf_temp, index = ['Actual NO', 'Actual YES'])\nmy_cmdf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"correct_percentage_log = 100 * (my_cm[0][0] + my_cm[1][1]) / predictions.shape[0]\nwrong_percentage = 100 - correct_percentage_log\nprint(f'Model Accuracy is: {correct_percentage_log:.4}%' )","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}